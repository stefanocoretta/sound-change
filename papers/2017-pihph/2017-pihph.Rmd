---
title: Modelling frequency effects in sound change chains
author:
- name: Stefano Coretta
  affiliation: The University of Manchester
  email: stefano.coretta@manchester.ac.uk
output: pihph::pihph_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Methods

Four simulations were implemented in R [@r-core-team2017] to test the behaviour of the model described in ...
The simulation starts with the creation of a lexicon of 100 lexical items.
50 words are assigned to the BART category, while the remaining 50 to the BAT category.
Within each category, lexical frequency from 1 to 50 is assigned to each lexical item, so that there are a BART and a BAT word of frequency 1, a BART and a BAT word of frequency 2, etc.
The lexicon is then populated using the specified formant values (6.5 and 5.5 Bark in this simulation).
A loop produces 50,000 total tokens among the lexical items.
The selection of the items is randomised and the weight of each item depends on its lexical frequency.
Higher frequency items (words) have thus more tokens in their representation than lower frequency words.

After the lexicon is initialised, a simulated sound change is applied to such lexicon.
The simulation goes through 200,000 iterations, in each of which a random lexical item is selected (lexical frequency weighing applies) and produced.
To produce the selected lexical item, a random target among the exemplars is chosen and produced with a random level of noise (here between -0.2 and +0.2).
If the selected word is a word containing the vowel which is undergoing change (the biased vowel), then a bias of -0.3 is applied to the formant value (on top of the noise).

Now that the word has been produced, an algorithm decides weather the word will be encoded back in memory or disregarded.
This will depend on its lexical frequency and most importantly on whether the token falls in an area of ambiguity, as in [@hay2015].
If the token is not ambiguous, i.e. if it is surrounded only by tokens of the same category, then the probability of being encoded back in memory is set to 1 (such a word is always sent back to memory).
If the token is ambiguous, i.e. if it falls in an area of overlap containing tokens of both categories, then the probability of being encoded in memory is set as a function of the lexical frequency of the relative word: p(x|M) = frequency(x) / maximum(frequency).
<!-- Need to make the equation and make it clearer -->
